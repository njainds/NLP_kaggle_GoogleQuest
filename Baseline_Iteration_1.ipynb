{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15f41e919e2f4f2faeb319be04d6d8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea396aeaacec4e7fa328fc5e2409f50c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e1eee03975714174bb3db4a6579c4bec",
              "IPY_MODEL_c98e52afb78245c494d73d89a8247e81"
            ]
          }
        },
        "ea396aeaacec4e7fa328fc5e2409f50c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1eee03975714174bb3db4a6579c4bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c822a355e37b4bc88186cc7a19925ad6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65c35f4c319245e4994d94f1eb30a27d"
          }
        },
        "c98e52afb78245c494d73d89a8247e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1e4c6124b0f493fb5638291445459bf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_be88c89ffced41e399343a67272e930f"
          }
        },
        "c822a355e37b4bc88186cc7a19925ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65c35f4c319245e4994d94f1eb30a27d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1e4c6124b0f493fb5638291445459bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "be88c89ffced41e399343a67272e930f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a83f5a67f80d421f8d87c7638d08aa59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d58020132b4e4290ba418f0a8ba1ae9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9b1744c3904e4b558e5134f44564dfea",
              "IPY_MODEL_55efba425480442d9f7efd7ff067bfd1"
            ]
          }
        },
        "d58020132b4e4290ba418f0a8ba1ae9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b1744c3904e4b558e5134f44564dfea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3492fa82c3c544269ef9fe2bf6992226",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab5b3e3ae18f45a3a18c7a7e9b28bbc3"
          }
        },
        "55efba425480442d9f7efd7ff067bfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_35898d222d254d5d9dbf94c5d873e186",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 361/361 [00:13&lt;00:00, 27.1B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c07f62574c514051bb226b95f25e4994"
          }
        },
        "3492fa82c3c544269ef9fe2bf6992226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab5b3e3ae18f45a3a18c7a7e9b28bbc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35898d222d254d5d9dbf94c5d873e186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c07f62574c514051bb226b95f25e4994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c53a1181cf540c687e2e04ee35e6830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f93a7b3b0a5b495eb9ea11dbe4a78054",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3b803540dec94801ba0353fdc7549d95",
              "IPY_MODEL_8909eced15254cf1b368e382d91653f1"
            ]
          }
        },
        "f93a7b3b0a5b495eb9ea11dbe4a78054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b803540dec94801ba0353fdc7549d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fe5447472cbb44358050795cacd66c78",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0fd7bf5f2f7c44c2bbed1ec1ec985d7b"
          }
        },
        "8909eced15254cf1b368e382d91653f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f436a024ecd74192a9d153bb68a793ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:12&lt;00:00, 34.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06a3ba4421324adfbe69ce4a0882ed13"
          }
        },
        "fe5447472cbb44358050795cacd66c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0fd7bf5f2f7c44c2bbed1ec1ec985d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f436a024ecd74192a9d153bb68a793ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06a3ba4421324adfbe69ce4a0882ed13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njainds/NLP_kaggle_GoogleQuest/blob/master/Baseline_Iteration_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "b63f58bf-c329-4b25-bc0a-cbf27663449b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/Google_Quest\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIS9Z-V5eJ17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\"\n",
        "#\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n",
        "#https://www.kaggle.com/phoenix9032/pytorch-bert-plain/data\n",
        "#https://huggingface.co/transformers/model_doc/bert.html?highlight=bertmodel#transformers.BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUUFcNA2p1Gv",
        "colab_type": "code",
        "outputId": "cefa94c0-2479-40fe-e054-4a06322b4461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Google_Quest\")\n",
        "os.getcwd()\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfwKNabMfUCe",
        "colab_type": "code",
        "outputId": "1c520357-22fb-4708-abc0-062df5d883e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "source": [
        "## Loading Libraries  -No they are not for books\n",
        "!pip install transformers\n",
        "import transformers, sys, os, gc\n",
        "import numpy as np, pandas as pd, math\n",
        "import torch, random, os, multiprocessing, glob\n",
        "import torch.nn.functional as F\n",
        "import torch, time\n",
        "\n",
        "from datasets.helpers.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
        "from scipy.stats import spearmanr\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, Dataset,RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
        "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from transformers.modeling_bert import BertPreTrainedModel \n",
        "from tqdm import tqdm\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\r\u001b[K     |▋                               | 10kB 33.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.26)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 20.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 34.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 66.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.26)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=cf32fc40db34115f176fe6defe31d4ccc4091e3a7a1cc98c2179e55831a31287\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM-XEXCzfVHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = 'datasets/google-quest-challenge/'\n",
        "## Make results reproducible .Else noone will believe you .\n",
        "import random\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqVYZYZ4gBI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PipeLineConfig:\n",
        "    def __init__(self, lr, warmup,accum_steps, epochs, seed, expname,head_tail,freeze,question_weight,answer_weight,fold,train):\n",
        "        self.lr = lr\n",
        "        self.warmup = warmup\n",
        "        self.accum_steps = accum_steps\n",
        "        self.epochs = epochs\n",
        "        self.seed = seed\n",
        "        self.expname = expname\n",
        "        self.head_tail = head_tail\n",
        "        self.freeze = freeze\n",
        "        self.question_weight = question_weight\n",
        "        self.answer_weight =answer_weight\n",
        "        self.fold = fold\n",
        "        self.train = train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TQfbPZngDo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_1 = PipeLineConfig(3e-5,0.05,4,0,42,'uncased_1',True,False,0.7,0.3,8,False)\n",
        "config = config_1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_An6zZgJvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_everything(config.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfV0bbt4gLrY",
        "colab_type": "code",
        "outputId": "4e09f139-b141-4a71-b78d-f784fa5d927c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## load the data \n",
        "train = pd.read_csv(ROOT+'train.csv')\n",
        "test = pd.read_csv(ROOT+'test.csv')\n",
        "sub = pd.read_csv(ROOT+'sample_submission.csv')\n",
        "train_len, test_len ,sub_len = len(train.index), len(test.index),len(sub.index)\n",
        "print(f'train size: {train_len}, test size: {test_len} , sample size: {sub_len}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 6079, test size: 476 , sample size: 476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdzOrciygN1M",
        "colab_type": "code",
        "outputId": "faf1d426-82fd-4519-d95f-bcdea049b31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "target_cols = sub.columns.tolist()[1:]\n",
        "len(target_cols)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGI-VNrsgdDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the Ref Kernel's\n",
        "from math import floor, ceil\n",
        "\n",
        "def _get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    \n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "        \n",
        "    segments = []\n",
        "    first_sep = True\n",
        "    current_segment_id = 0\n",
        "    \n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            if first_sep:\n",
        "                first_sep = False \n",
        "            else:\n",
        "                current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    \n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def _trim_input(title, question, answer, max_sequence_length=290, t_max_len=30, q_max_len=128, a_max_len=128):\n",
        "    \n",
        "    #350+128+30 = 508 + 4 = 512\n",
        "    \n",
        "    t = tokenizer.tokenize(title)\n",
        "    q = tokenizer.tokenize(question)\n",
        "    a = tokenizer.tokenize(answer)\n",
        "    \n",
        "    t_len = len(t)\n",
        "    q_len = len(q)\n",
        "    a_len = len(a)\n",
        "\n",
        "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
        "        \n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
        "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "      \n",
        "        if a_max_len > a_len:\n",
        "            a_new_len = a_len \n",
        "            q_new_len = q_max_len + (a_max_len - a_len)\n",
        "        elif q_max_len > q_len:\n",
        "            a_new_len = a_max_len + (q_max_len - q_len)\n",
        "            q_new_len = q_len\n",
        "        else:\n",
        "            a_new_len = a_max_len\n",
        "            q_new_len = q_max_len\n",
        "            \n",
        "            \n",
        "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
        "        q_len_head = round(q_new_len/2)\n",
        "        q_len_tail = -1* (q_new_len -q_len_head)\n",
        "        a_len_head = round(a_new_len/2)\n",
        "        a_len_tail = -1* (a_new_len -a_len_head)        ## Head+Tail method .\n",
        "        t = t[:t_new_len]\n",
        "        if config.head_tail :\n",
        "            q = q[:q_len_head]+q[q_len_tail:]\n",
        "            a = a[:a_len_head]+a[a_len_tail:]\n",
        "        else:\n",
        "            q = q[:q_new_len]\n",
        "            a = a[:a_new_len] ## No Head+Tail ,usual processing\n",
        "    \n",
        "    return t, q, a\n",
        "\n",
        "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "   # stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
        "\n",
        "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
        "    input_masks = _get_masks(stoken, max_sequence_length)\n",
        "    input_segments = _get_segments(stoken, max_sequence_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "def _get_stoken_output(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "    return stoken\n",
        "\n",
        "def compute_input_tokens(df, columns, tokenizer, max_sequence_length):\n",
        "    \n",
        "    input_tokens, input_masks, input_segments = [], [], []\n",
        "    for _, instance in df[columns].iterrows():\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length)\n",
        "        tokens= _get_stoken_output(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_tokens.append(tokens)\n",
        "    return input_tokens\n",
        "\n",
        "def compute_input_arays(df, columns, tokenizer, max_sequence_length,t_max_len=30, q_max_len=128, a_max_len=128):\n",
        "    \n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in df[columns].iterrows():\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length,t_max_len, q_max_len, a_max_len)\n",
        "        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    return [\n",
        "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
        "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
        "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
        "    ]\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIqUfn6XgjlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, lengths, labels = None):\n",
        "        \n",
        "        self.inputs = inputs\n",
        "        if labels is not None:\n",
        "            self.labels = labels\n",
        "        else:\n",
        "            self.labels = None\n",
        "        self.lengths = lengths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        input_ids       = self.inputs[0][idx]\n",
        "        input_masks     = self.inputs[1][idx]\n",
        "        input_segments  = self.inputs[2][idx]\n",
        "        lengths         = self.lengths[idx]\n",
        "        if self.labels is not None: # targets\n",
        "            labels = self.labels[idx]\n",
        "            return input_ids, input_masks, input_segments, labels, lengths\n",
        "        return input_ids, input_masks, input_segments, lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YG6sXicgnU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomBert(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(CustomBert, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.bn = nn.BatchNorm1d(1024)\n",
        "        self.linear  = nn.Linear(config.hidden_size,1024)\n",
        "        self.classifier = nn.Linear(1024, self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        lin_output = F.relu(self.bn(self.linear(pooled_output))) ## Note : This Linear layer is added without expert supervision . This will worsen the results . \n",
        "                                               ## But you are smarter than me , so you will figure out,how to customize better.\n",
        "        lin_output = self.dropout(lin_output)    \n",
        "        logits = self.classifier(lin_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKeoTkDAgu3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "15f41e919e2f4f2faeb319be04d6d8c6",
            "ea396aeaacec4e7fa328fc5e2409f50c",
            "e1eee03975714174bb3db4a6579c4bec",
            "c98e52afb78245c494d73d89a8247e81",
            "c822a355e37b4bc88186cc7a19925ad6",
            "65c35f4c319245e4994d94f1eb30a27d",
            "b1e4c6124b0f493fb5638291445459bf",
            "be88c89ffced41e399343a67272e930f",
            "a83f5a67f80d421f8d87c7638d08aa59",
            "d58020132b4e4290ba418f0a8ba1ae9f",
            "9b1744c3904e4b558e5134f44564dfea",
            "55efba425480442d9f7efd7ff067bfd1",
            "3492fa82c3c544269ef9fe2bf6992226",
            "ab5b3e3ae18f45a3a18c7a7e9b28bbc3",
            "35898d222d254d5d9dbf94c5d873e186",
            "c07f62574c514051bb226b95f25e4994",
            "5c53a1181cf540c687e2e04ee35e6830",
            "f93a7b3b0a5b495eb9ea11dbe4a78054",
            "3b803540dec94801ba0353fdc7549d95",
            "8909eced15254cf1b368e382d91653f1",
            "fe5447472cbb44358050795cacd66c78",
            "0fd7bf5f2f7c44c2bbed1ec1ec985d7b",
            "f436a024ecd74192a9d153bb68a793ea",
            "06a3ba4421324adfbe69ce4a0882ed13"
          ]
        },
        "outputId": "83230c6b-bb4b-4280-f4eb-7c040b5f1f5b"
      },
      "source": [
        "#test\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "outputs = model(input_ids)\n",
        "\n",
        "#outputs[3][12].shape\n",
        "len(outputs) #4\n",
        "outputs[0].shape #torch.Size([1, 8, 768])\n",
        "outputs[1].shape #torch.Size([1, 768])\n",
        "len(outputs[2]) #tuple of 13 elements each of torch.Size([1, 768])\n",
        "len(outputs[3]) #tuple of 12 elements each of torch.Size([1, 12, 8, 8])\n",
        "\n",
        "#format of output: ((tensor),(tensor),((tensor)*13),((tensor)*12))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15f41e919e2f4f2faeb319be04d6d8c6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a83f5a67f80d421f8d87c7638d08aa59",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c53a1181cf540c687e2e04ee35e6830",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akg-2sSChGA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train fct\n",
        "\n",
        "def train_model(train_loader, optimizer, criterion, scheduler,  config):\n",
        "  model.train()\n",
        "  avg_losst = 0\n",
        "  avg_loss1 = 0\n",
        "  avg_loss2 = 0\n",
        "  avg_loss = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  for idx, batch in enumerate(train_loader):\n",
        "    input_ids, input_masks, input_segments, labels, _ = batch\n",
        "    input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)\n",
        "    outputs = model(input_ids = input_ids.long(), attention_masks = input_masks, token_type_ids = input_segments, labels=None)\n",
        "    logits = outputs[0]\n",
        "    tot_loss = criterion(logits, labels)\n",
        "    q_loss = criterion(logits[:21], labels[:21])\n",
        "    a_loss = criterion(logits[21:], labels[21:])\n",
        "    loss = config.question_weight*q_loss + config.answer_weight*a_loss\n",
        "    loss.backward()\n",
        "    if (idx+1) % config.accum_steps ==0:\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    avg_losst += tot_loss.item()/len(train_loader)\n",
        "    avg_loss1 += q_loss.item()/len(train_loader)\n",
        "    avg_loss2 += a_loss.item()/len(train_loader)\n",
        "    avg_loss += loss.item()/len(train_loader)\n",
        "    del input_ids, input_masks, input_segments, labels\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return avg_losst, avg_loss1, avg_loss2, avg_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO97Y4XhrPYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_model(val_loader, val_shape, batch_size=8):\n",
        "  avg_val_loss = 0\n",
        "  model.eval()\n",
        "  val_preds = np.zeros((val_shape,len(target_cols)))  \n",
        "  originals = np.zeros((val_shape,len(target_cols)))\n",
        "  with torch.no_grad():\n",
        "    for idx, batch in enumerate(val_loader):\n",
        "      input_ids, input_masks, input_segments, labels, _ = batch\n",
        "      val_outputs = model(input_ids = input_ids.long(), attention_masks = input_masks, token_type_ids = input_segments, labels=None)\n",
        "      logits = val_outputs[0]\n",
        "      avg_val_loss += criterion(logits, labels).item()/len(val_loader)\n",
        "      val_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
        "      originals[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
        "\n",
        "    score = 0\n",
        "    preds = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
        "    rho_val = np.mean([spearmanr(original[:i],preds[:i]).correlation for i in range(preds.shape[1])])\n",
        "    print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
        "  return avg_val_loss, rho_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnlp0wWgr78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict\n",
        "\n",
        "def predict_result(model, test_loader, batch_size):\n",
        "  model.eval()\n",
        "  preds = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "  tz = tqdm(enumerate(test_loader))\n",
        "  for idx, batch in tz:\n",
        "    with torch.no_grad():\n",
        "      test_out = model(inputs_ids = batch[0].to(device), attention_masks = batch[1].to(device), labels = None, token_type_ids = batch[2].to(device))\n",
        "      test_out = test_out[0]\n",
        "      preds[idx*batch_size : (idx+1)*batch_size]  = test_out.detach().cpu().squeeze().numpy()\n",
        "  \n",
        "  output  = torch.sigmoid(torch.tensor(preds)).numpy()\n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUdFHxEirFwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "5f1b1651-767b-475d-ef41-057e71bfc5f3"
      },
      "source": [
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<torch.cuda.device object at 0x7fc271bf8c88>\n",
            "1\n",
            "Tesla P100-PCIE-16GB\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN0d_1iDj09X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ff010f15-e341-4109-ea33-bb6ef1a92003"
      },
      "source": [
        "# Prepare inputs for model training\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "input_categories = list(train.columns[[1,2,5]])\n",
        "input_categories\n",
        "\n",
        "bert_config = BertConfig.from_json_file('./datasets/transformers/bert-base-uncased-config.json')\n",
        "bert_config.num_labels = len(target_cols)\n",
        "bert_config.output_attentions = True\n",
        "bert_config.output_hidden_states = True\n",
        "\n",
        "bert_model = 'bert-base-uncased'\n",
        "do_lower_case = 'uncased' in bert_model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "output_model_file = 'plain_pytorch_model1.bin'\n",
        "\n",
        "test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239)\n",
        "lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
        "lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
        "\n",
        "test_set = QuestDataset(test_inputs, lengths_test, labels=None)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "results = np.zeros((len(test),len(target_cols)))\n",
        "\n",
        "print(results.shape)\n",
        "print(len(test_loader))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(476, 30)\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ17g5u9sXNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4a984d74-bed2-4ff1-ce80-a36703f81d55"
      },
      "source": [
        "# Train the model and evaluate\n",
        "\n",
        "num_folds = config.fold\n",
        "seed  = config.seed\n",
        "batch_size = 8\n",
        "epochs = config.epochs\n",
        "accum_steps  = 1\n",
        "\n",
        "kf = MultilabelStratifiedKFold(n_splits = num_folds, random_state = seed)\n",
        "y_train = train[target_cols].values\n",
        "\n",
        "if config.train:\n",
        "  for fold, (train_idx, val_idx) in enumerate(kf.split(train.values, y_train)):\n",
        "    if fold>0:\n",
        "      break\n",
        "    print(\"Current fold is : \", fold)\n",
        "    \n",
        "    train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "    print(\"train shape is %s and val shape is %s\" % (train_df.shape, val_df.shape))\n",
        "    \n",
        "    inputs_train = compute_input_arays(train_df, input_categories, max_sequence_length=512, t_max_len=30, q_max_len=239,a_max_len=239)\n",
        "    outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
        "    outputs_train = torch.tensor(outputs_train,dtype=torch.float32)\n",
        "    lengths_train = np.argmax(inputs_train[0] == 0, axis=1)\n",
        "    lengths_train[lengths_train == 0] = inputs_train[0].shape[1] \n",
        "    train_set = QuestDataset(inputs_train, lenghts_train, labels = outputs_train)\n",
        "    train_sampler = RandomSampler(train_set)\n",
        "    train_loader = DataLoader(train_set, batch_size = batch_size, sampler=train_sampler)\n",
        "\n",
        "    inputs_val = compute_input_arays(val_df, input_categories, max_sequence_length=512, t_max_len=30, q_max_len=239,a_max_len=239)\n",
        "    outputs_val = compute_output_arrays(val_df, columns=target_cols)\n",
        "    outputs_val = torch.tensor(outputs_val,dtype=torch.float32)\n",
        "    lengths_val = np.argmax(inputs_val[0] == 0, axis=1)\n",
        "    lengths_val[lengths_val == 0] = inputs_val[0].shape[1] \n",
        "    val_set = QuestDataset(inputs_val, lenghts_val, labels = outputs_val)\n",
        "    val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    model = CustomBert.from_pretrained(bert_model, config = bert_config)\n",
        "    model.zero_grad()\n",
        "    model.to(device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    if config.freeze:\n",
        "      for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.train()\n",
        "\n",
        "    i=0\n",
        "    best_avg_loss = 100\n",
        "    best_score = -1\n",
        "    best_param_loss = None\n",
        "    best_param_score = None\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias','LayerNorm.bias','LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "                                    {'params':[p for n,p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                                     'weight_decay': 0.8\n",
        "                                     },\n",
        "                                    {'params':[p for n,p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                                     'weight_decay': 0\n",
        "                                     }\n",
        "                                    ]\n",
        "    optimizer = torch.optim.AdamW(optimizer.optimizer_grouped_parameters, lr = config.lr, eps = 4e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//accum_steps)\n",
        "    print(\"##### Training ######\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      torch.cuda.empty_cache()\n",
        "      start_time = time.time()\n",
        "      avg_losst, avg_loss1, avg_loss2, avg_loss = train_model(train_loader, optimizer, criterion, scheduler, config)\n",
        "      avg_val_loss, score = val_model(val_loader, val_df.shape[0])\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(\"Epoch {}/{} \\t\n",
        "       train_loss_overall = {:.4f} \\t train_loss = {:.4f} \\t train_loss_Question = {:.4f} \\t train_loss_Answer = {:.4f} \\t\n",
        "       val_loss_overall = {:.4f} \\t score={:.6f} \\t time = {:.2f}s\".format(epoch+1, epochs, avg_losst,avg_loss, avg_loss1, avg_loss2, \n",
        "                                                         avg_val_loss,score, elapsed_time))\n",
        "      if best_avg_loss>avg_val_loss:\n",
        "        i=0\n",
        "        best_avg_loss = avg_val_loss\n",
        "        best_param_loss = model.state_dict()\n",
        "\n",
        "      if best_score < score\n",
        "      best_score = score\n",
        "      best_param_score = model.state_dict()\n",
        "      print(\"best_param_score_{}_{}.pt\".format(config.expname, fold+1))\n",
        "      torch.save(best_param_score, \"best_param_score_{}_{}.pt\".format(config.expname, fold+1))\n",
        "      else:\n",
        "        i=i+1\n",
        "\n",
        "    model.load_state_dict(best_param_score)\n",
        "    result += predict_result(model, test_loader)/num_folds\n",
        "\n",
        "  del train_df, val_df, model, optimizer, scheduler, criterion, val_loader, train_loader, test_loader, val_set, train_set\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()   \n",
        "\n",
        "\n",
        "\n",
        "print(\"Final result: %s\", %(result.shape))\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JKlXXw5BgvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv(r'../input/google-quest-challenge/sample_submission.csv')\n",
        "submission.loc[:, 'question_asker_intent_understanding':] = result\n",
        "#submission.loc[~submission['qa_id'].isin(qa_id_list),'question_type_spelling']=0.0\n",
        "#submission.loc[submission['qa_id'].isin(qa_id_list),'question_type_spelling'] = 1.0\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}