{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12b6458237584fa784de4c2970e4d0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a2dcdfbed8864707b4a00b560b1b6026",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2eae60d505f44639c353289cebb2e76",
              "IPY_MODEL_d7b4187b6c4e45a2818587f6e0f1f900"
            ]
          }
        },
        "a2dcdfbed8864707b4a00b560b1b6026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2eae60d505f44639c353289cebb2e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9020cd06388945d6ac6f66e0a4544636",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73a9e8919ad54441a72ffe514f8863c0"
          }
        },
        "d7b4187b6c4e45a2818587f6e0f1f900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82be40ced9b24e20ad7b01bb56baa6a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 340kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_502c5921a04a4a2ab0116e2e511b8ac4"
          }
        },
        "9020cd06388945d6ac6f66e0a4544636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73a9e8919ad54441a72ffe514f8863c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82be40ced9b24e20ad7b01bb56baa6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "502c5921a04a4a2ab0116e2e511b8ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njainds/NLP_kaggle_GoogleQuest/blob/master/Baseline_Iteration_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "d8ee69e1-e642-4cfa-aef9-9c6a482af75a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/Google_Quest\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "best_param_score_uncased_1_1.pt  datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIS9Z-V5eJ17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\"\n",
        "#\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n",
        "#https://www.kaggle.com/phoenix9032/pytorch-bert-plain/data\n",
        "#https://huggingface.co/transformers/model_doc/bert.html?highlight=bertmodel#transformers.BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUUFcNA2p1Gv",
        "colab_type": "code",
        "outputId": "8876b0ed-d0f8-417a-9659-59887dda0890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Google_Quest\")\n",
        "os.getcwd()\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best_param_score_uncased_1_1.pt  datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfwKNabMfUCe",
        "colab_type": "code",
        "outputId": "7bd7dd18-fa52-4757-9e13-5acd41a81f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "source": [
        "## Loading Libraries  -No they are not for books\n",
        "!pip install transformers\n",
        "import transformers, sys, os, gc\n",
        "import numpy as np, pandas as pd, math\n",
        "import torch, random, os, multiprocessing, glob\n",
        "import torch.nn.functional as F\n",
        "import torch, time\n",
        "\n",
        "from datasets.helpers.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
        "from scipy.stats import spearmanr\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, Dataset,RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
        "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from transformers.modeling_bert import BertPreTrainedModel \n",
        "from tqdm import tqdm\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\r\u001b[K     |▋                               | 10kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 27.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 23.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 13.2MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 10.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 9.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 9.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 26.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 34.5MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 39.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 42.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 44.7MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 47.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 48.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 48.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 49.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 49.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 49.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 49.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 49.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 49.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 49.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 49.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 49.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 49.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.26)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 85kB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 30.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.26)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=d158d1b2e224783c112f44805992133a9d8b9c876b928edda27154a12ab81195\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM-XEXCzfVHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = 'datasets/google-quest-challenge/'\n",
        "## Make results reproducible .Else noone will believe you .\n",
        "import random\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqVYZYZ4gBI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PipeLineConfig:\n",
        "    def __init__(self, lr, warmup,accum_steps, epochs, seed, expname,head_tail,freeze,question_weight,answer_weight,fold,train):\n",
        "        self.lr = lr\n",
        "        self.warmup = warmup\n",
        "        self.accum_steps = accum_steps\n",
        "        self.epochs = epochs\n",
        "        self.seed = seed\n",
        "        self.expname = expname\n",
        "        self.head_tail = head_tail\n",
        "        self.freeze = freeze\n",
        "        self.question_weight = question_weight\n",
        "        self.answer_weight =answer_weight\n",
        "        self.fold = fold\n",
        "        self.train = train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TQfbPZngDo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_1 = PipeLineConfig(3e-5,0.05,1,2,42,'uncased_1',True,False,0.7,0.3,5,True)\n",
        "config = config_1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_An6zZgJvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_everything(config.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfV0bbt4gLrY",
        "colab_type": "code",
        "outputId": "a9cd51a0-75c9-4d63-890e-f0e2b5f48f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## load the data \n",
        "train = pd.read_csv(ROOT+'train.csv')\n",
        "test = pd.read_csv(ROOT+'test.csv')\n",
        "sub = pd.read_csv(ROOT+'sample_submission.csv')\n",
        "train_len, test_len ,sub_len = len(train.index), len(test.index),len(sub.index)\n",
        "print(f'train size: {train_len}, test size: {test_len} , sample size: {sub_len}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 6079, test size: 476 , sample size: 476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdzOrciygN1M",
        "colab_type": "code",
        "outputId": "1357a7fb-3356-4663-b49f-4c46dd49dc13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "target_cols = sub.columns.tolist()[1:]\n",
        "len(target_cols)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGI-VNrsgdDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the Ref Kernel's\n",
        "from math import floor, ceil\n",
        "\n",
        "def _get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    \n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "        \n",
        "    segments = []\n",
        "    first_sep = True\n",
        "    current_segment_id = 0\n",
        "    \n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            if first_sep:\n",
        "                first_sep = False \n",
        "            else:\n",
        "                current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    \n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def _trim_input(title, question, answer, max_sequence_length=290, t_max_len=30, q_max_len=128, a_max_len=128):\n",
        "    \n",
        "    #350+128+30 = 508 + 4 = 512\n",
        "    \n",
        "    t = tokenizer.tokenize(title)\n",
        "    q = tokenizer.tokenize(question)\n",
        "    a = tokenizer.tokenize(answer)\n",
        "    \n",
        "    t_len = len(t)\n",
        "    q_len = len(q)\n",
        "    a_len = len(a)\n",
        "\n",
        "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
        "        \n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
        "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "      \n",
        "        if a_max_len > a_len:\n",
        "            a_new_len = a_len \n",
        "            q_new_len = q_max_len + (a_max_len - a_len)\n",
        "        elif q_max_len > q_len:\n",
        "            a_new_len = a_max_len + (q_max_len - q_len)\n",
        "            q_new_len = q_len\n",
        "        else:\n",
        "            a_new_len = a_max_len\n",
        "            q_new_len = q_max_len\n",
        "            \n",
        "            \n",
        "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
        "        q_len_head = round(q_new_len/2)\n",
        "        q_len_tail = -1* (q_new_len -q_len_head)\n",
        "        a_len_head = round(a_new_len/2)\n",
        "        a_len_tail = -1* (a_new_len -a_len_head)        ## Head+Tail method .\n",
        "        t = t[:t_new_len]\n",
        "        if config.head_tail :\n",
        "            q = q[:q_len_head]+q[q_len_tail:]\n",
        "            a = a[:a_len_head]+a[a_len_tail:]\n",
        "        else:\n",
        "            q = q[:q_new_len]\n",
        "            a = a[:a_new_len] ## No Head+Tail ,usual processing\n",
        "    \n",
        "    return t, q, a\n",
        "\n",
        "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "   # stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
        "\n",
        "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
        "    input_masks = _get_masks(stoken, max_sequence_length)\n",
        "    input_segments = _get_segments(stoken, max_sequence_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "def _get_stoken_output(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "    return stoken\n",
        "\n",
        "def compute_input_tokens(df, columns, tokenizer, max_sequence_length):\n",
        "    \n",
        "    input_tokens, input_masks, input_segments = [], [], []\n",
        "    for _, instance in df[columns].iterrows():\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length)\n",
        "        tokens= _get_stoken_output(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_tokens.append(tokens)\n",
        "    return input_tokens\n",
        "\n",
        "def compute_input_arays(df, columns, tokenizer, max_sequence_length,t_max_len=30, q_max_len=128, a_max_len=128):\n",
        "    \n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in df[columns].iterrows():\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length,t_max_len, q_max_len, a_max_len)\n",
        "        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    return [\n",
        "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
        "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
        "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
        "    ]\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIqUfn6XgjlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, lengths, labels = None):\n",
        "        \n",
        "        self.inputs = inputs\n",
        "        if labels is not None:\n",
        "            self.labels = labels\n",
        "        else:\n",
        "            self.labels = None\n",
        "        self.lengths = lengths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        input_ids       = self.inputs[0][idx]\n",
        "        input_masks     = self.inputs[1][idx]\n",
        "        input_segments  = self.inputs[2][idx]\n",
        "        lengths         = self.lengths[idx]\n",
        "        if self.labels is not None: # targets\n",
        "            labels = self.labels[idx]\n",
        "            return input_ids, input_masks, input_segments, labels, lengths\n",
        "        return input_ids, input_masks, input_segments, lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YG6sXicgnU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomBert(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(CustomBert, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.bn = nn.BatchNorm1d(1024)\n",
        "        self.linear  = nn.Linear(config.hidden_size,1024)\n",
        "        self.classifier = nn.Linear(1024, self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        lin_output = F.relu(self.bn(self.linear(pooled_output))) ## Note : This Linear layer is added without expert supervision . This will worsen the results . \n",
        "                                               ## But you are smarter than me , so you will figure out,how to customize better.\n",
        "        lin_output = self.dropout(lin_output)    \n",
        "        logits = self.classifier(lin_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKeoTkDAgu3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test\n",
        "#from transformers import BertModel, BertTokenizer\n",
        "#import torch\n",
        "\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "#input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "#outputs = model(input_ids)\n",
        "\n",
        "#outputs[3][12].shape\n",
        "#len(outputs) #4\n",
        "#outputs[0].shape #torch.Size([1, 8, 768])\n",
        "#outputs[1].shape #torch.Size([1, 768])\n",
        "#len(outputs[2]) #tuple of 13 elements each of torch.Size([1, 768])\n",
        "#len(outputs[3]) #tuple of 12 elements each of torch.Size([1, 12, 8, 8])\n",
        "\n",
        "#format of output: ((tensor),(tensor),((tensor)*13),((tensor)*12))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akg-2sSChGA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train fct\n",
        "\n",
        "def train_model(train_loader, optimizer, criterion, scheduler,  config):\n",
        "  model.train()\n",
        "  avg_losst = 0\n",
        "  avg_loss1 = 0\n",
        "  avg_loss2 = 0\n",
        "  avg_loss = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  for idx, batch in enumerate(train_loader):\n",
        "    input_ids, input_masks, input_segments, labels, _ = batch\n",
        "    input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)\n",
        "    outputs = model(input_ids = input_ids.long(), attention_mask = input_masks, token_type_ids = input_segments, labels=None)\n",
        "    logits = outputs[0]\n",
        "    tot_loss = criterion(logits, labels)\n",
        "    q_loss = criterion(logits[:21], labels[:21])\n",
        "    a_loss = criterion(logits[21:], labels[21:])\n",
        "    loss = config.question_weight*q_loss + config.answer_weight*a_loss\n",
        "    loss.backward()\n",
        "    if (idx+1) % config.accum_steps ==0:\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    avg_losst += tot_loss.item()/len(train_loader)\n",
        "    avg_loss1 += q_loss.item()/len(train_loader)\n",
        "    avg_loss2 += a_loss.item()/len(train_loader)\n",
        "    avg_loss += loss.item()/len(train_loader)\n",
        "    del input_ids, input_masks, input_segments, labels\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return avg_losst, avg_loss1, avg_loss2, avg_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO97Y4XhrPYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_model(val_loader, val_shape, batch_size):\n",
        "  avg_val_loss = 0\n",
        "  model.eval()\n",
        "  val_preds = np.zeros((val_shape,len(target_cols)))  \n",
        "  originals = np.zeros((val_shape,len(target_cols)))\n",
        "  with torch.no_grad():\n",
        "    for idx, batch in enumerate(val_loader):\n",
        "      input_ids, input_masks, input_segments, labels, _ = batch\n",
        "      input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)\n",
        "      val_outputs = model(input_ids = input_ids.long(), attention_mask = input_masks, token_type_ids = input_segments, labels=None)\n",
        "      logits = val_outputs[0]\n",
        "      avg_val_loss += criterion(logits, labels).item()/len(val_loader)\n",
        "      val_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
        "      originals[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
        "\n",
        "    score = 0\n",
        "    preds = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
        "    rho_val = np.mean([spearmanr(originals[:,i],preds[:,i]).correlation for i in range(preds.shape[1])])\n",
        "    print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
        "  return avg_val_loss, rho_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnlp0wWgr78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict\n",
        "\n",
        "def predict_result(model, test_loader, batch_size):\n",
        "  model.eval()\n",
        "  preds = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "  tz = tqdm(enumerate(test_loader))\n",
        "  for idx, batch in tz:\n",
        "    with torch.no_grad():\n",
        "      test_out = model(input_ids = batch[0].to(device), attention_mask = batch[1].to(device), labels = None, token_type_ids = batch[2].to(device))\n",
        "      test_out = test_out[0]\n",
        "      preds[idx*batch_size : (idx+1)*batch_size]  = test_out.detach().cpu().squeeze().numpy()\n",
        "  \n",
        "  output  = torch.sigmoid(torch.tensor(preds)).numpy()\n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUdFHxEirFwL",
        "colab_type": "code",
        "outputId": "b6aab8c3-246e-4791-969c-38ab6ea7633b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<torch.cuda.device object at 0x7f37dcb44400>\n",
            "1\n",
            "Tesla P4\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN0d_1iDj09X",
        "colab_type": "code",
        "outputId": "93dafa32-70e7-4836-98c9-6d73dba9377d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "12b6458237584fa784de4c2970e4d0d4",
            "a2dcdfbed8864707b4a00b560b1b6026",
            "b2eae60d505f44639c353289cebb2e76",
            "d7b4187b6c4e45a2818587f6e0f1f900",
            "9020cd06388945d6ac6f66e0a4544636",
            "73a9e8919ad54441a72ffe514f8863c0",
            "82be40ced9b24e20ad7b01bb56baa6a7",
            "502c5921a04a4a2ab0116e2e511b8ac4"
          ]
        }
      },
      "source": [
        "# Prepare inputs for model training\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "input_categories = list(train.columns[[1,2,5]])\n",
        "input_categories\n",
        "\n",
        "bert_config = BertConfig.from_json_file('./datasets/transformers/bert-base-uncased-config.json')\n",
        "bert_config.num_labels = len(target_cols)\n",
        "bert_config.output_attentions = True\n",
        "bert_config.output_hidden_states = True\n",
        "\n",
        "bert_model = 'bert-base-uncased'\n",
        "do_lower_case = 'uncased' in bert_model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "output_model_file = 'plain_pytorch_model1.bin'\n",
        "\n",
        "test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239)\n",
        "lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
        "lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
        "\n",
        "test_set = QuestDataset(test_inputs, lengths_test, labels=None)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "results = np.zeros((len(test),len(target_cols)))\n",
        "\n",
        "print(results.shape)\n",
        "print(len(test_loader))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12b6458237584fa784de4c2970e4d0d4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(476, 30)\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ17g5u9sXNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model and evaluate\n",
        "\n",
        "num_folds = config.fold\n",
        "seed  = config.seed\n",
        "batch_size = 4\n",
        "epochs = config.epochs\n",
        "accum_steps  = 1\n",
        "result = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "kf = MultilabelStratifiedKFold(n_splits = num_folds, random_state = seed)\n",
        "y_train = train[target_cols].values\n",
        "\n",
        "if config.train:\n",
        "  for fold, (train_idx, val_idx) in enumerate(kf.split(train.values, y_train)):\n",
        "    if fold>0:\n",
        "      break\n",
        "    print(\"Current fold is : \", fold)\n",
        "    \n",
        "    train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "    print(\"train shape is %s and val shape is %s\" % (train_df.shape, val_df.shape))\n",
        "    \n",
        "    inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239,a_max_len=239)\n",
        "    outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
        "    outputs_train = torch.tensor(outputs_train,dtype=torch.float32)\n",
        "    lengths_train = np.argmax(inputs_train[0] == 0, axis=1)\n",
        "    lengths_train[lengths_train == 0] = inputs_train[0].shape[1] \n",
        "    train_set = QuestDataset(inputs_train, lengths_train, labels = outputs_train)\n",
        "    train_sampler = RandomSampler(train_set)\n",
        "    train_loader = DataLoader(train_set, batch_size = batch_size, sampler=train_sampler)\n",
        "\n",
        "    inputs_val = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239,a_max_len=239)\n",
        "    outputs_val = compute_output_arrays(val_df, columns=target_cols)\n",
        "    outputs_val = torch.tensor(outputs_val,dtype=torch.float32)\n",
        "    lengths_val = np.argmax(inputs_val[0] == 0, axis=1)\n",
        "    lengths_val[lengths_val == 0] = inputs_val[0].shape[1] \n",
        "    val_set = QuestDataset(inputs_val, lengths_val, labels = outputs_val)\n",
        "    val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    model = CustomBert.from_pretrained(bert_model, config = bert_config)\n",
        "    model.zero_grad()\n",
        "    model.to(device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    if config.freeze:\n",
        "      for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.train()\n",
        "\n",
        "    i=0\n",
        "    best_avg_loss = 100\n",
        "    best_score = -1\n",
        "    best_param_loss = None\n",
        "    best_param_score = None\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias','LayerNorm.bias','LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [{'params':[p for n,p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                                     'weight_decay': 0.8},{'params':[p for n,p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                                     'weight_decay': 0}]\n",
        "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr = config.lr, eps = 4e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//accum_steps)\n",
        "    print(\"##### Training ######\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      print(\"fold # {} and epoch # {}\".format(fold,epoch))\n",
        "      torch.cuda.empty_cache()\n",
        "      start_time = time.time()\n",
        "      avg_losst, avg_loss1, avg_loss2, avg_loss = train_model(train_loader, optimizer, criterion, scheduler, config)\n",
        "      avg_val_loss, score = val_model(val_loader, val_df.shape[0], batch_size=batch_size)\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(\"Epoch {}/{} \\t train_loss_overall = {:.4f} \\t train_loss = {:.4f} \\t train_loss_Question = {:.4f} \\t train_loss_Answer = {:.4f} \\t val_loss_overall = {:.4f} \\t score={:.6f} \\t time = {:.2f}s\".format(epoch+1, epochs, avg_losst,avg_loss, avg_loss1, avg_loss2, avg_val_loss,score, elapsed_time))\n",
        "      if best_avg_loss>avg_val_loss:\n",
        "        i=0\n",
        "        best_avg_loss = avg_val_loss\n",
        "        best_param_loss = model.state_dict()\n",
        "\n",
        "      if best_score < score:\n",
        "        best_score = score\n",
        "        best_param_score = model.state_dict()\n",
        "        print(\"best_param_score_{}_{}.pt\".format(config.expname, fold+1))\n",
        "        torch.save(best_param_score, \"best_param_score_{}_{}.pt\".format(config.expname, fold+1))\n",
        "      else:\n",
        "        i=i+1\n",
        "\n",
        "    model.load_state_dict(best_param_score)\n",
        "    result += predict_result(model, test_loader, batch_size=batch_size)/num_folds\n",
        "\n",
        "  del train_df, val_df, model, optimizer, scheduler, criterion, val_loader, train_loader, test_loader, val_set, train_set\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()   \n",
        "\n",
        "\n",
        "\n",
        "print(\"Final result: %s\" %(result.shape))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JKlXXw5BgvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#submission = pd.read_csv(r'../input/google-quest-challenge/sample_submission.csv')\n",
        "#submission.loc[:, 'question_asker_intent_understanding':] = result\n",
        "#submission.loc[~submission['qa_id'].isin(qa_id_list),'question_type_spelling']=0.0\n",
        "#submission.loc[submission['qa_id'].isin(qa_id_list),'question_type_spelling'] = 1.0\n",
        "\n",
        "#submission.to_csv('submission.csv', index=False)\n",
        "#submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}