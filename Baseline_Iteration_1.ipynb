{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e2137af89ad9443e87cd8ba84cb0c820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a0952ab67735463a9a86f3cd8bd83755",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_665498ed9d3640c3874d49d579e2e05e",
              "IPY_MODEL_c9f1b8e2845f409c961d624a05581cdc"
            ]
          }
        },
        "a0952ab67735463a9a86f3cd8bd83755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "665498ed9d3640c3874d49d579e2e05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f16b8fe1201046c2addb6a53d015a063",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29be5de8e0d441bebd64fb6b482bd141"
          }
        },
        "c9f1b8e2845f409c961d624a05581cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_84895679fce243b7b1f30c6298356b63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 361/361 [00:06&lt;00:00, 53.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b0dcb5fbd7c4af0bd8fc2fb55a40b3b"
          }
        },
        "f16b8fe1201046c2addb6a53d015a063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29be5de8e0d441bebd64fb6b482bd141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84895679fce243b7b1f30c6298356b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b0dcb5fbd7c4af0bd8fc2fb55a40b3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "988499f8c81e45a896774a90f1c568fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0d3c90a94bb241f7beffb8ee5a907e18",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c3f7e0c567754a74a7dd0a5bed0f027d",
              "IPY_MODEL_34533fa2b5de48d895ee435c84d725e2"
            ]
          }
        },
        "0d3c90a94bb241f7beffb8ee5a907e18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3f7e0c567754a74a7dd0a5bed0f027d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_86b68528dc354dd8a74754783ec5d6fc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c10ae6f1e214bb09db2634ec35dfa88"
          }
        },
        "34533fa2b5de48d895ee435c84d725e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8385627f56d04c8f9b5a60038d830965",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [02:19&lt;00:00, 3.15MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fe89fa5e23546659edf4add6a00a773"
          }
        },
        "86b68528dc354dd8a74754783ec5d6fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c10ae6f1e214bb09db2634ec35dfa88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8385627f56d04c8f9b5a60038d830965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fe89fa5e23546659edf4add6a00a773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njainds/NLP_kaggle_GoogleQuest/blob/master/Baseline_Iteration_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "dd6dda0b-2e40-44ad-9278-789f3f556b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/Google_Quest\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIS9Z-V5eJ17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\"\n",
        "#\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n",
        "#https://www.kaggle.com/phoenix9032/pytorch-bert-plain/data\n",
        "#https://huggingface.co/transformers/model_doc/bert.html?highlight=bertmodel#transformers.BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUUFcNA2p1Gv",
        "colab_type": "code",
        "outputId": "f341bf41-5ab4-4597-f5b6-64e6a8d937f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Google_Quest\")\n",
        "os.getcwd()\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfwKNabMfUCe",
        "colab_type": "code",
        "outputId": "52647336-2d65-4031-89bc-ff1dbcd5695f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "source": [
        "## Loading Libraries  -No they are not for books\n",
        "!pip install transformers\n",
        "import transformers, sys, os, gc\n",
        "import numpy as np, pandas as pd, math\n",
        "import torch, random, os, multiprocessing, glob\n",
        "import torch.nn.functional as F\n",
        "import torch, time\n",
        "\n",
        "from datasets.helpers.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
        "from scipy.stats import spearmanr\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, Dataset,RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
        "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from transformers.modeling_bert import BertPreTrainedModel \n",
        "from tqdm import tqdm\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\r\u001b[K     |▋                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 34.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 39.0MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 40.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 43.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 44.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 37.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 38.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 40.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 34.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 37.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 45.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 52.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 57.5MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 62.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 65.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 66.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 67.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 63.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 63.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 63.3MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 63.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 63.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 63.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 63.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 63.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 63.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 63.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.26)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 61.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.26)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=7e593d5d5c5a44f0ecc66f24653fe4e58825254fa1403ea3e57f045937a8cfec\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM-XEXCzfVHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = 'datasets/google-quest-challenge/'\n",
        "## Make results reproducible .Else noone will believe you .\n",
        "import random\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqVYZYZ4gBI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PipeLineConfig:\n",
        "    def __init__(self, lr, warmup,accum_steps, epochs, seed, expname,head_tail,freeze,question_weight,answer_weight,fold,train):\n",
        "        self.lr = lr\n",
        "        self.warmup = warmup\n",
        "        self.accum_steps = accum_steps\n",
        "        self.epochs = epochs\n",
        "        self.seed = seed\n",
        "        self.expname = expname\n",
        "        self.head_tail = head_tail\n",
        "        self.freeze = freeze\n",
        "        self.question_weight = question_weight\n",
        "        self.answer_weight =answer_weight\n",
        "        self.fold = fold\n",
        "        self.train = train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TQfbPZngDo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_1 = PipeLineConfig(3e-5,0.05,4,0,42,'uncased_1',True,False,0.7,0.3,8,False)\n",
        "config = config_1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_An6zZgJvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_everything(config.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfV0bbt4gLrY",
        "colab_type": "code",
        "outputId": "ffd3b18c-17eb-4c1a-97c4-9afcf57cee16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## load the data \n",
        "train = pd.read_csv(ROOT+'train.csv')\n",
        "test = pd.read_csv(ROOT+'test.csv')\n",
        "sub = pd.read_csv(ROOT+'sample_submission.csv')\n",
        "train_len, test_len ,sub_len = len(train.index), len(test.index),len(sub.index)\n",
        "print(f'train size: {train_len}, test size: {test_len} , sample size: {sub_len}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 6079, test size: 476 , sample size: 476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdzOrciygN1M",
        "colab_type": "code",
        "outputId": "88697052-6f9e-4448-a4a6-72d19c30cacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "target_cols = sub.columns.tolist()[1:]\n",
        "len(target_cols)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGI-VNrsgdDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the Ref Kernel's\n",
        "from math import floor, ceil\n",
        "\n",
        "def _get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    \n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "        \n",
        "    segments = []\n",
        "    first_sep = True\n",
        "    current_segment_id = 0\n",
        "    \n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            if first_sep:\n",
        "                first_sep = False \n",
        "            else:\n",
        "                current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    \n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def _trim_input(title, question, answer, max_sequence_length=290, t_max_len=30, q_max_len=128, a_max_len=128):\n",
        "    \n",
        "    #350+128+30 = 508 + 4 = 512\n",
        "    \n",
        "    t = tokenizer.tokenize(title)\n",
        "    q = tokenizer.tokenize(question)\n",
        "    a = tokenizer.tokenize(answer)\n",
        "    \n",
        "    t_len = len(t)\n",
        "    q_len = len(q)\n",
        "    a_len = len(a)\n",
        "\n",
        "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
        "        \n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
        "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "      \n",
        "        if a_max_len > a_len:\n",
        "            a_new_len = a_len \n",
        "            q_new_len = q_max_len + (a_max_len - a_len)\n",
        "        elif q_max_len > q_len:\n",
        "            a_new_len = a_max_len + (q_max_len - q_len)\n",
        "            q_new_len = q_len\n",
        "        else:\n",
        "            a_new_len = a_max_len\n",
        "            q_new_len = q_max_len\n",
        "            \n",
        "            \n",
        "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
        "        q_len_head = round(q_new_len/2)\n",
        "        q_len_tail = -1* (q_new_len -q_len_head)\n",
        "        a_len_head = round(a_new_len/2)\n",
        "        a_len_tail = -1* (a_new_len -a_len_head)        ## Head+Tail method .\n",
        "        t = t[:t_new_len]\n",
        "        if config.head_tail :\n",
        "            q = q[:q_len_head]+q[q_len_tail:]\n",
        "            a = a[:a_len_head]+a[a_len_tail:]\n",
        "        else:\n",
        "            q = q[:q_new_len]\n",
        "            a = a[:a_new_len] ## No Head+Tail ,usual processing\n",
        "    \n",
        "    return t, q, a\n",
        "\n",
        "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "   # stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
        "\n",
        "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
        "    input_masks = _get_masks(stoken, max_sequence_length)\n",
        "    input_segments = _get_segments(stoken, max_sequence_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "def _get_stoken_output(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "    return stoken\n",
        "\n",
        "def compute_input_tokens(df, columns, tokenizer, max_sequence_length):\n",
        "    \n",
        "    input_tokens, input_masks, input_segments = [], [], []\n",
        "    for _, instance in df[columns].iterrows():\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length)\n",
        "        tokens= _get_stoken_output(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_tokens.append(tokens)\n",
        "    return input_tokens\n",
        "\n",
        "def compute_input_arays(df, columns, tokenizer, max_sequence_length,t_max_len=30, q_max_len=128, a_max_len=128):\n",
        "    \n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in df[columns].iterrows():\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length,t_max_len, q_max_len, a_max_len)\n",
        "        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    return [\n",
        "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
        "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
        "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
        "    ]\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIqUfn6XgjlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, lengths, labels = None):\n",
        "        \n",
        "        self.inputs = inputs\n",
        "        if labels is not None:\n",
        "            self.labels = labels\n",
        "        else:\n",
        "            self.labels = None\n",
        "        self.lengths = lengths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        input_ids       = self.inputs[0][idx]\n",
        "        input_masks     = self.inputs[1][idx]\n",
        "        input_segments  = self.inputs[2][idx]\n",
        "        lengths         = self.lengths[idx]\n",
        "        if self.labels is not None: # targets\n",
        "            labels = self.labels[idx]\n",
        "            return input_ids, input_masks, input_segments, labels, lengths\n",
        "        return input_ids, input_masks, input_segments, lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YG6sXicgnU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomBert(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(CustomBert, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.bn = nn.BatchNorm1d(1024)\n",
        "        self.linear  = nn.Linear(config.hidden_size,1024)\n",
        "        self.classifier = nn.Linear(1024, self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        lin_output = F.relu(self.bn(self.linear(pooled_output))) ## Note : This Linear layer is added without expert supervision . This will worsen the results . \n",
        "                                               ## But you are smarter than me , so you will figure out,how to customize better.\n",
        "        lin_output = self.dropout(lin_output)    \n",
        "        logits = self.classifier(lin_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKeoTkDAgu3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "e2137af89ad9443e87cd8ba84cb0c820",
            "a0952ab67735463a9a86f3cd8bd83755",
            "665498ed9d3640c3874d49d579e2e05e",
            "c9f1b8e2845f409c961d624a05581cdc",
            "f16b8fe1201046c2addb6a53d015a063",
            "29be5de8e0d441bebd64fb6b482bd141",
            "84895679fce243b7b1f30c6298356b63",
            "3b0dcb5fbd7c4af0bd8fc2fb55a40b3b",
            "988499f8c81e45a896774a90f1c568fe",
            "0d3c90a94bb241f7beffb8ee5a907e18",
            "c3f7e0c567754a74a7dd0a5bed0f027d",
            "34533fa2b5de48d895ee435c84d725e2",
            "86b68528dc354dd8a74754783ec5d6fc",
            "3c10ae6f1e214bb09db2634ec35dfa88",
            "8385627f56d04c8f9b5a60038d830965",
            "2fe89fa5e23546659edf4add6a00a773"
          ]
        },
        "outputId": "c1ae72ad-abb2-4e08-dc7d-f52e627384f4"
      },
      "source": [
        "#test\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "outputs = model(input_ids)\n",
        "\n",
        "#outputs[3][12].shape\n",
        "len(outputs) #4\n",
        "outputs[0].shape #torch.Size([1, 8, 768])\n",
        "outputs[1].shape #torch.Size([1, 768])\n",
        "len(outputs[2]) #tuple of 13 elements each of torch.Size([1, 768])\n",
        "len(outputs[3]) #tuple of 12 elements each of torch.Size([1, 12, 8, 8])\n",
        "\n",
        "#format of output: ((tensor),(tensor),((tensor)*13),((tensor)*12))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2137af89ad9443e87cd8ba84cb0c820",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "988499f8c81e45a896774a90f1c568fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akg-2sSChGA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train fct\n",
        "\n",
        "def train_model(train_loader, optimizer, criterion, scheduler,  config):\n",
        "  model.train()\n",
        "  avg_losst = 0\n",
        "  avg_loss1 = 0\n",
        "  avg_loss2 = 0\n",
        "  avg_loss = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  for idx, batch in enumerate(train_loader):\n",
        "    input_ids, input_masks, input_segments, labels, _ = batch\n",
        "    input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)\n",
        "    outputs = model(input_ids = input_ids.long(), attention_masks = input_masks, token_type_ids = input_segments, labels=None)\n",
        "    logits = outputs[0]\n",
        "    tot_loss = criterion(logits, labels)\n",
        "    q_loss = criterion(logits[:21], labels[:21])\n",
        "    a_loss = criterion(logits[21:], labels[21:])\n",
        "    loss = config.question_weight*q_loss + config.answer_weight*a_loss\n",
        "    loss.backward()\n",
        "    if (idx+1) % config.accum_steps ==0:\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    avg_losst += tot_loss.item()/len(train_loader)\n",
        "    avg_loss1 += q_loss.item()/len(train_loader)\n",
        "    avg_loss2 += a_loss.item()/len(train_loader)\n",
        "    avg_loss += loss.item()/len(train_loader)\n",
        "    del input_ids, input_masks, input_segments, labels\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return avg_losst, avg_loss1, avg_loss2, avg_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO97Y4XhrPYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_model(val_loader, val_shape, batch_size=8):\n",
        "  avg_val_loss = 0\n",
        "  model.eval()\n",
        "  val_preds = np.zeros((val_shape,len(target_cols)))  \n",
        "  originals = np.zeros((val_shape,len(target_cols)))\n",
        "  with torch.no_grad():\n",
        "    for idx, batch in enumerate(val_loader):\n",
        "      input_ids, input_masks, input_segments, labels, _ = batch\n",
        "      val_outputs = model(input_ids = input_ids.long(), attention_masks = input_masks, token_type_ids = input_segments, labels=None)\n",
        "      logits = val_outputs[0]\n",
        "      avg_val_loss += criterion(logits, labels).item()/len(val_loader)\n",
        "      val_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
        "      originals[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
        "\n",
        "    score = 0\n",
        "    preds = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
        "    rho_val = np.mean([spearmanr(original[:i],preds[:i]).correlation for i in range(preds.shape[1])])\n",
        "    print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
        "  return avg_val_loss, rho_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnlp0wWgr78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict\n",
        "\n",
        "def predict_result(model, test_loader, batch_size):\n",
        "  model.eval()\n",
        "  preds = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "  tz = tqdm(enumerate(test_loader))\n",
        "  for idx, batch in tz:\n",
        "    with torch.no_grad():\n",
        "      test_out = model(inputs_ids = batch[0].to(device), attention_masks = batch[1].to(device), labels = None, token_type_ids = batch[2].to(device))\n",
        "      test_out = test_out[0]\n",
        "      preds[idx*batch_size : (idx+1)*batch_size]  = test_out.detach().cpu().squeeze().numpy()\n",
        "  \n",
        "  output  = torch.sigmoid(torch.tensor(preds)).numpy()\n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUdFHxEirFwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "da6382f1-7bfc-4f60-8b43-cc897cd459e1"
      },
      "source": [
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<torch.cuda.device object at 0x7fe639f6ecf8>\n",
            "1\n",
            "Tesla T4\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN0d_1iDj09X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a7f5c5db-aadd-413b-9ae5-04e0130dc620"
      },
      "source": [
        "# Prepare inputs for model training\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "input_categories = list(train.columns[[1,2,5]])\n",
        "input_categories\n",
        "\n",
        "bert_config = BertConfig.from_json_file('./datasets/transformers/bert-base-uncased-config.json')\n",
        "bert_config.num_labels = len(target_cols)\n",
        "\n",
        "bert_model = 'bert-base-uncased'\n",
        "do_lower_case = 'uncased' in bert_model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "output_model_file = 'plain_pytorch_model1.bin'\n",
        "\n",
        "test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239)\n",
        "lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
        "lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
        "\n",
        "test_set = QuestDataset(test_inputs, lengths_test, labels=None)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "results = np.zeros((len(test),len(target_cols)))\n",
        "\n",
        "print(results.shape)\n",
        "print(len(test_loader))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(476, 30)\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ17g5u9sXNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model and evaluate\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}